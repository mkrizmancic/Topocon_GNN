{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362afeae",
   "metadata": {},
   "source": [
    "# Imports, settings & definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf30794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import pathlib\n",
    "import functools\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from gnn_fiedler_approx.algebraic_connectivity_script import load_dataset, generate_model, generate_loss_function, evaluate\n",
    "from gnn_fiedler_approx.algebraic_connectivity_evaluate import combine_data_objects\n",
    "\n",
    "ON_HPC = \"PBS_O_HOME\" in os.environ\n",
    "HPC_MODEL_DIR = pathlib.Path(\"/lustre/home/mkrizman/Topocon_GNN/gnn_fiedler_approx/models/\")\n",
    "LOC_MODEL_DIR = pathlib.Path().cwd().parent / \"models\"\n",
    "\n",
    "api = wandb.Api()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "__builtins__.device = device  # Hack to make device available in other modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab425bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=None)\n",
    "def load_dataset_cached(selected_graph_sizes, selected_features, transform, batch_size, split):\n",
    "    if selected_graph_sizes is not None:\n",
    "        selected_graph_sizes = {selected_graph_sizes: -1}\n",
    "    return load_dataset(\n",
    "        selected_graph_sizes=selected_graph_sizes,\n",
    "        selected_features=selected_features,\n",
    "        label_normalization=None,\n",
    "        transform=transform,\n",
    "        batch_size=batch_size,\n",
    "        split=split,\n",
    "        suppress_output=True,\n",
    "    )\n",
    "\n",
    "def prepare_data(run, dataset_definition):\n",
    "    # Load the dataset.\n",
    "    if dataset_definition == \"standard\":\n",
    "        print(\"standard\")\n",
    "        train_data_obj, val_data_obj, test_data_obj, dataset_config, features, dataset_props = load_dataset_cached(\n",
    "            selected_graph_sizes=None,\n",
    "            selected_features=tuple(run.config[\"selected_features\"]),\n",
    "            transform=run.config[\"transform\"],\n",
    "            batch_size=run.config[\"dataset\"][\"batch_size\"],\n",
    "            split=tuple(run.config[\"dataset\"][\"split\"]),\n",
    "        )\n",
    "\n",
    "        datasets_for_evaluation = [\"test\", \"entire\"]\n",
    "        dataobjects_for_evaluation = [test_data_obj, combine_data_objects([train_data_obj, val_data_obj, test_data_obj])]\n",
    "\n",
    "    elif dataset_definition == \"generalization\":\n",
    "        train_data_obj = None\n",
    "\n",
    "        train_data_obj, val_data_obj, test_data_obj, dataset_config, features, dataset_props = load_dataset_cached(\n",
    "            selected_graph_sizes=None,\n",
    "            selected_features=tuple(run.config[\"selected_features\"]),\n",
    "            transform=run.config[\"transform\"],\n",
    "            batch_size=run.config[\"dataset\"][\"batch_size\"],\n",
    "            split=tuple(run.config[\"dataset\"][\"split\"]),\n",
    "        )\n",
    "\n",
    "        _, _, test_data_obj_15, dataset_config, features, dataset_props = load_dataset_cached(\n",
    "            selected_graph_sizes=\"11-15_mix_200\",\n",
    "            selected_features=tuple(run.config[\"selected_features\"]),\n",
    "            transform=run.config[\"transform\"],\n",
    "            batch_size=run.config[\"dataset\"][\"batch_size\"],\n",
    "            split=(0.0, 0.0),\n",
    "        )\n",
    "\n",
    "        _, _, test_data_obj_20, dataset_config, features, dataset_props = load_dataset_cached(\n",
    "            selected_graph_sizes=\"16-20_mix_200\",\n",
    "            selected_features=tuple(run.config[\"selected_features\"]),\n",
    "            transform=run.config[\"transform\"],\n",
    "            batch_size=run.config[\"dataset\"][\"batch_size\"],\n",
    "            split=(0.0, 0.0),\n",
    "        )\n",
    "\n",
    "        _, _, test_data_obj_25, dataset_config, features, dataset_props = load_dataset_cached(\n",
    "            selected_graph_sizes=\"21-25_mix_200\",\n",
    "            selected_features=tuple(run.config[\"selected_features\"]),\n",
    "            transform=run.config[\"transform\"],\n",
    "            batch_size=run.config[\"dataset\"][\"batch_size\"],\n",
    "            split=(0.0, 0.0),\n",
    "        )\n",
    "\n",
    "        _, _, test_data_obj_50, dataset_config, features, dataset_props = load_dataset_cached(\n",
    "            selected_graph_sizes=\"50_mix_200\",\n",
    "            selected_features=tuple(run.config[\"selected_features\"]),\n",
    "            transform=run.config[\"transform\"],\n",
    "            batch_size=run.config[\"dataset\"][\"batch_size\"],\n",
    "            split=(0.0, 0.0),\n",
    "        )\n",
    "\n",
    "        datasets_for_evaluation = [\"test\", \"15\", \"20\", \"25\", \"50\", \"entire\"]\n",
    "        dataobjects_for_evaluation = [\n",
    "            test_data_obj,\n",
    "            test_data_obj_15,\n",
    "            test_data_obj_20,\n",
    "            test_data_obj_25,\n",
    "            test_data_obj_50,\n",
    "        ]\n",
    "        dataobjects_for_evaluation.append(combine_data_objects(dataobjects_for_evaluation))\n",
    "\n",
    "    return train_data_obj, datasets_for_evaluation, dataobjects_for_evaluation, dataset_props\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d9fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_run(run, dataset_definition):\n",
    "    train_data_obj, datasets_for_evaluation, dataobjects_for_evaluation, dataset_props = prepare_data(run, dataset_definition)\n",
    "\n",
    "    # Generate the model skeleton.\n",
    "    model = generate_model(\n",
    "        architecture=run.config[\"architecture\"],\n",
    "        in_channels=dataset_props[\"feature_dim\"],\n",
    "        hidden_channels=run.config[\"hidden_channels\"],\n",
    "        gnn_layers=run.config[\"gnn_layers\"],\n",
    "        mlp_layers=run.config[\"mlp_layers\"],\n",
    "        pool=run.config[\"pool\"],\n",
    "        jk=run.config[\"jk\"],\n",
    "        dropout=run.config[\"dropout\"],\n",
    "        norm=run.config[\"norm\"],\n",
    "        act=run.config[\"activation\"],\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # Download or locate the model dict.\n",
    "    if ON_HPC:\n",
    "        model_dict_path = HPC_MODEL_DIR / f\"{run.id}_best_model.pth\"\n",
    "    else:\n",
    "        model_dict_path = LOC_MODEL_DIR / f\"{run.id}_best_model.pth\"\n",
    "        if not model_dict_path.exists():\n",
    "            os.system(f\"scp mkrizman@login-gpu.hpc.srce.hr:{HPC_MODEL_DIR}/{run.id}_best_model.pth {LOC_MODEL_DIR}/\")\n",
    "\n",
    "    # Load the model state dict.\n",
    "    model.load_state_dict(torch.load(model_dict_path, map_location=device)[\"model_state_dict\"])\n",
    "\n",
    "    criterion = generate_loss_function(run.config.get(\"loss\", \"MAPE\"))\n",
    "\n",
    "\n",
    "    results_table = dict.fromkeys(datasets_for_evaluation)\n",
    "    results_scores = dict.fromkeys(datasets_for_evaluation)\n",
    "    epoch = -1\n",
    "\n",
    "    print(f\"    {run.id}\")\n",
    "    for dataset, eval_data_obj in zip(datasets_for_evaluation, dataobjects_for_evaluation):\n",
    "        eval_results = evaluate(\n",
    "                model,\n",
    "                epoch,\n",
    "                criterion,\n",
    "                train_data_obj,\n",
    "                eval_data_obj,\n",
    "                dataset_props[\"transformation\"],\n",
    "                title=f\"Results on the {dataset} dataset\",\n",
    "                plot_graphs_wandb=False,\n",
    "                plot_embeddings=False,\n",
    "                make_table_wandb=False,\n",
    "                suppress_output=True\n",
    "            )\n",
    "\n",
    "        print(f\"        {dataset}\")\n",
    "        if dataset == \"test\":\n",
    "            print(f\"            mean_err: {eval_results['mean_err']:.5f} | {run.summary.get('mean_err', 'N/A'):.5f}\")\n",
    "            print(f\"            stddev_err: {eval_results['stddev_err']:.5f} | {run.summary.get('stddev_err', 'N/A'):.5f}\")\n",
    "            print(f\"            eval_train_loss: {eval_results['eval_train_loss']:.5f} | {run.summary.get('eval_train_loss', 'N/A'):.5f}\")\n",
    "            print(f\"            eval_test_loss: {eval_results['eval_test_loss']:.5f} | {run.summary.get('eval_test_loss', 'N/A'):.5f}\")\n",
    "            print(f\"            good_within.99: {eval_results['good_within']['99']:.5f} | {run.summary['good_within']['99']:.5f}\")\n",
    "            print(f\"            good_within.95: {eval_results['good_within']['95']:.5f} | {run.summary['good_within']['95']:.5f}\")\n",
    "            print(f\"            good_within.90: {eval_results['good_within']['90']:.5f} | {run.summary['good_within']['90']:.5f}\")\n",
    "            print(f\"            good_within.80: {eval_results['good_within']['80']:.5f} | {run.summary['good_within']['80']:.5f}\")\n",
    "\n",
    "        results_table[dataset] = eval_results[\"table\"]\n",
    "        results_scores[dataset] = eval_results[\"good_within\"]\n",
    "\n",
    "    return results_table, results_scores, datasets_for_evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a73268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_baseline(run, dataset_definition, template_tables):\n",
    "    train_data_obj, datasets_for_evaluation, dataobjects_for_evaluation, dataset_props = prepare_data(run, dataset_definition)\n",
    "\n",
    "    avg = torch.mean(train_data_obj.y)\n",
    "    avg = avg.item()\n",
    "\n",
    "    results_table = dict.fromkeys(datasets_for_evaluation)\n",
    "    results_scores = dict.fromkeys(datasets_for_evaluation)\n",
    "    for dataset in datasets_for_evaluation:\n",
    "        df = template_tables[dataset].copy()\n",
    "\n",
    "        df[\"Predicted\"] = avg\n",
    "\n",
    "        # Calculate the statistics.\n",
    "        df[\"Error\"] = df[\"True\"] - df[\"Predicted\"]\n",
    "        df[\"Error %\"] = 100 * df[\"Error\"] / df[\"True\"]\n",
    "        df[\"abs(Error)\"] = np.abs(df[\"Error\"])\n",
    "\n",
    "        good_within = {\n",
    "            \"99\": len(df[df[\"Error %\"].between(-1, 1)]) / len(df) * 100,\n",
    "            \"95\": len(df[df[\"Error %\"].between(-5, 5)]) / len(df) * 100,\n",
    "            \"90\": len(df[df[\"Error %\"].between(-10, 10)]) / len(df) * 100,\n",
    "            \"80\": len(df[df[\"Error %\"].between(-20, 20)]) / len(df) * 100,\n",
    "        }\n",
    "\n",
    "        results_table[dataset] = df\n",
    "        results_scores[dataset] = good_within\n",
    "\n",
    "    return results_table, results_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71965cc0",
   "metadata": {},
   "source": [
    "# Evaluate final models on the dataset and store results for all graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ec4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = pathlib.Path().cwd().parent / \"results\" / \"runs_data\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## Standard evaluation with full and relaxed models on \"in-distribution\" data.\n",
    "# sweep_ids = {\n",
    "#     \"Full\": \"lo3tkjor\",\n",
    "#     \"Dist. 32\": \"i6swq9gx\",\n",
    "#     \"Dist. 64\": \"vuezsfvg\",\n",
    "# }\n",
    "# save_path /= \"results_final_evaluation.pkl\"\n",
    "# save_ext = \"\"\n",
    "# dataset_definition = \"standard\"\n",
    "\n",
    "## Evaluation of the full model on \"out-of-distribution\" data to test generalization.\n",
    "sweep_ids = {\n",
    "    \"Full\": \"lo3tkjor\",\n",
    "}\n",
    "save_path /= \"results_final_generalization.pkl\"\n",
    "save_ext = \"_gen\"\n",
    "dataset_definition = \"generalization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c3f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all_table = dict.fromkeys(sweep_ids.keys())\n",
    "results_all_scores = dict.fromkeys(sweep_ids.keys())\n",
    "for model_type, sweep_id in sweep_ids.items():\n",
    "    sweep = api.sweep(f\"marko-krizmancic/gnn_fiedler_approx_v3/{sweep_id}\")  # labels_all\n",
    "    runs = sweep.runs\n",
    "\n",
    "    results_all_table[model_type] = {}\n",
    "    results_all_scores[model_type] = {}\n",
    "    print(f\"Evaluating {model_type} models...\")\n",
    "    for run in runs:\n",
    "        print(f\"    {run.id}\")\n",
    "        table, scores, datasets_for_evaluation = evaluate_run(run, dataset_definition)\n",
    "        results_all_table[model_type][run.id] = table\n",
    "        results_all_scores[model_type][run.id] = scores\n",
    "\n",
    "table, scores = evaluate_baseline(runs[0], dataset_definition, results_all_table[model_type][runs[0].id])\n",
    "results_all_table[\"Baseline\"] = {runs[0].id: table}\n",
    "results_all_scores[\"Baseline\"] = {runs[0].id: scores}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a939ca",
   "metadata": {},
   "source": [
    "### Save or load evaluated data to avoid computing it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f010a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if save_path.exists():\n",
    "    x = input(f\"Are you sure you want to overwrite {save_path.name}? (y/n): \")\n",
    "    if x.lower() != \"y\":\n",
    "       print(\"Aborting save.\")\n",
    "       exit()\n",
    "\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump((results_all_table, results_all_scores, datasets_for_evaluation), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f4a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(save_path, \"rb\") as f:\n",
    "    results_all_table, results_all_scores, datasets_for_evaluation = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c5031e",
   "metadata": {},
   "source": [
    "# Plot average error distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f167386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# --- Zoom window parameters ---\n",
    "zoom_x_range = [0, 5]\n",
    "zoom_y_range = [50, 100]\n",
    "zoom_width = 0.5\n",
    "zoom_height = 0.5\n",
    "zoom_x_pos = 0.25\n",
    "zoom_y_pos = 0.15\n",
    "\n",
    "def paper_to_data(coord, range):\n",
    "    return coord * (range[1] - range[0]) + range[0]\n",
    "\n",
    "def data_to_paper(coord, range):\n",
    "    return (coord - range[0]) / (range[1] - range[0])\n",
    "# --- End of parameters ---\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = {\n",
    "    \"Full\": \"blue\",\n",
    "    \"Dist. 32\": \"green\",\n",
    "    \"Dist. 64\": \"red\",\n",
    "    \"Baseline\": \"#FFA500\"  # Orange\n",
    "}\n",
    "\n",
    "line_styles = {  # ['solid', 'dot', 'dash', 'longdash', 'dashdot', 'longdashdot']\n",
    "    \"test\": \"solid\",\n",
    "    \"entire\": \"dash\",\n",
    "    \"15\": \"dot\",\n",
    "    \"20\": \"longdash\",\n",
    "    \"25\": \"dashdot\",\n",
    "    \"50\": \"longdashdot\",\n",
    "}\n",
    "\n",
    "# Define a common x-axis for interpolation\n",
    "x_common = np.linspace(0, 100, 501)\n",
    "xaxis_range = [0, 100]\n",
    "yaxis_range = [0, 105]\n",
    "\n",
    "for dataset in datasets_for_evaluation:\n",
    "    for model_type in results_all_table:\n",
    "\n",
    "        list_of_runs = results_all_table[model_type].keys()\n",
    "        if not list_of_runs:\n",
    "            continue\n",
    "\n",
    "        interpolated_ecdfs = []\n",
    "        for run_id in list_of_runs:\n",
    "            if dataset not in results_all_table[model_type][run_id]:\n",
    "                continue\n",
    "\n",
    "            df = results_all_table[model_type][run_id][dataset]\n",
    "\n",
    "            # Calculate ECDF for the current run\n",
    "            plot_df = pd.DataFrame()\n",
    "            plot_df[\"abs(Error %)\"] = np.abs(df[\"Error %\"])\n",
    "            plot_df.sort_values(by=\"abs(Error %)\", inplace=True)\n",
    "\n",
    "            x_ecdf = plot_df[\"abs(Error %)\"]\n",
    "            y_ecdf = (np.arange(1, len(plot_df) + 1)) / len(plot_df) * 100\n",
    "\n",
    "            # Interpolate the ECDF on the common x-axis\n",
    "            y_interp = np.interp(x_common, x_ecdf, y_ecdf, right=100)\n",
    "            interpolated_ecdfs.append(y_interp)\n",
    "\n",
    "        if not interpolated_ecdfs:\n",
    "            continue\n",
    "\n",
    "        # Calculate mean, min, and max of the interpolated ECDFs\n",
    "        all_ecdfs = np.vstack(interpolated_ecdfs)\n",
    "        mean_ecdf = np.mean(all_ecdfs, axis=0)\n",
    "        min_ecdf = np.min(all_ecdfs, axis=0)\n",
    "        max_ecdf = np.max(all_ecdfs, axis=0)\n",
    "\n",
    "        # --- Main plot traces ---\n",
    "        # Plot the average line\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x_common,\n",
    "            y=mean_ecdf,\n",
    "            mode='lines',\n",
    "            name=f\"{model_type} - {dataset}\",\n",
    "            line=dict(color=colors[model_type], dash=line_styles[dataset])\n",
    "        ))\n",
    "\n",
    "        if colors[model_type].startswith(\"#\"):\n",
    "            fillcolor = f\"rgba({int(colors[model_type].lstrip('#')[0:2], 16)}, {int(colors[model_type].lstrip('#')[2:4], 16)}, {int(colors[model_type].lstrip('#')[4:6], 16)}, 0.2)\"\n",
    "        else:\n",
    "            color_options = {'blue': '0,0,255', 'green': '0,128,0', 'red': '255,0,0'}\n",
    "            fillcolor = f\"rgba({color_options[colors[model_type]]}, 0.2)\"\n",
    "\n",
    "        # Plot the shaded area for min/max spread\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.concatenate([x_common, x_common[::-1]]),\n",
    "            y=np.concatenate([min_ecdf, max_ecdf[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor=fillcolor,\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            hoverinfo=\"skip\",\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "        # --- Zoom plot traces ---\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x_common,\n",
    "            y=mean_ecdf,\n",
    "            mode='lines',\n",
    "            line=dict(color=colors[model_type], dash=line_styles[dataset]),\n",
    "            xaxis='x2',\n",
    "            yaxis='y2',\n",
    "            showlegend=False,\n",
    "            hoverinfo=\"skip\"\n",
    "        ))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.concatenate([x_common, x_common[::-1]]),\n",
    "            y=np.concatenate([min_ecdf, max_ecdf[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor=fillcolor,\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            hoverinfo=\"skip\",\n",
    "            showlegend=False,\n",
    "            xaxis='x2',\n",
    "            yaxis='y2'\n",
    "        ))\n",
    "\n",
    "\n",
    "fig.update_xaxes(showspikes=True, tickvals=[1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], title_text=\"Absolute Error (%)\")\n",
    "fig.update_yaxes(showspikes=True, nticks=10, title_text=\"% of graphs within error\")\n",
    "\n",
    "fig.update_layout(\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=5, r=5, t=5, b=65),\n",
    "    legend=dict(\n",
    "            title=dict(text=\"Model - Dataset\", side=\"top\"),\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        ),\n",
    "    xaxis_range=xaxis_range,\n",
    "    yaxis_range=yaxis_range,\n",
    "    xaxis2=dict(\n",
    "        domain=[zoom_x_pos, zoom_x_pos + zoom_width],\n",
    "        range=zoom_x_range,\n",
    "        showticklabels=True,\n",
    "        tickfont=dict(size=12),\n",
    "        ticks=\"inside\",\n",
    "        tickmode=\"linear\",\n",
    "        tick0=zoom_x_range[0],\n",
    "        dtick=1,\n",
    "        anchor=\"y2\"\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        domain=[zoom_y_pos, zoom_y_pos + zoom_height],\n",
    "        range=zoom_y_range,\n",
    "        showticklabels=True,\n",
    "        tickfont=dict(size=14),\n",
    "        ticks=\"inside\",\n",
    "        tickmode=\"linear\",\n",
    "        tick0=zoom_y_range[0],\n",
    "        dtick=10,\n",
    "        anchor=\"x2\"\n",
    "    ),\n",
    "    shapes=[\n",
    "        # Shape to indicate zoomed area on the main plot\n",
    "        dict(\n",
    "            type=\"rect\",\n",
    "            xref=\"x\", yref=\"y\",\n",
    "            x0=zoom_x_range[0], y0=zoom_y_range[0],\n",
    "            x1=zoom_x_range[1], y1=zoom_y_range[1],\n",
    "            line=dict(color=\"rgba(0,0,0,0.5)\", width=1, dash=\"dot\"),\n",
    "            fillcolor=\"rgba(0,0,0,0.1)\"\n",
    "        ),\n",
    "        # Shape for the border of the zoom window\n",
    "        dict(\n",
    "            type='rect',\n",
    "            xref='paper', yref='paper',\n",
    "            x0=zoom_x_pos, y0=zoom_y_pos,\n",
    "            x1=zoom_x_pos + zoom_width, y1=zoom_y_pos + zoom_height,\n",
    "            line=dict(color='black', width=1)\n",
    "        ),\n",
    "        # Line connecting lower-left corners\n",
    "        dict(\n",
    "            type=\"line\",\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x0=data_to_paper(zoom_x_range[0], xaxis_range), y0=data_to_paper(zoom_y_range[0], yaxis_range),\n",
    "            x1=zoom_x_pos, y1=zoom_y_pos,\n",
    "            line=dict(color=\"rgba(0,0,0,0.5)\", width=1, dash=\"dot\")\n",
    "        ),\n",
    "        # Line connecting upper-right corners\n",
    "        dict(\n",
    "            type=\"line\",\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x0=data_to_paper(zoom_x_range[1], xaxis_range), y0=data_to_paper(zoom_y_range[1], yaxis_range),\n",
    "            x1=zoom_x_pos + zoom_width, y1=zoom_y_pos + zoom_height,\n",
    "            line=dict(color=\"rgba(0,0,0,0.5)\", width=1, dash=\"dot\")\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "image_path = pathlib.Path().cwd().parent / \"results\" / \"error_plots\"\n",
    "image_path.mkdir(parents=True, exist_ok=True)\n",
    "image_path /= f\"final_comparison{save_ext}.pdf\"\n",
    "fig.write_image(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ea8ea0",
   "metadata": {},
   "source": [
    "# Plot accuracies per graph size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69576ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot showing percentage of graphs with absolute error <= 1% and <= 5%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "colors = {\n",
    "    \"Full\": \"blue\",\n",
    "    \"Dist. 32\": \"green\",\n",
    "    \"Dist. 64\": \"red\",\n",
    "    \"Baseline\": \"#FFA500\"  # Orange\n",
    "}\n",
    "\n",
    "# Group data by number of nodes and calculate percentages\n",
    "def calculate_error_percentages(df):\n",
    "    results = []\n",
    "\n",
    "    for nodes in sorted(df['Nodes'].unique()):\n",
    "        node_data = df[df['Nodes'] == nodes]\n",
    "        total_count = len(node_data)\n",
    "\n",
    "        # Calculate absolute error percentages\n",
    "        abs_error = np.abs(node_data['Error %'])\n",
    "\n",
    "        # Count graphs with absolute error <= 1% and <= 5%\n",
    "        count_1_percent = np.sum(abs_error <= 1.0)\n",
    "        count_5_percent = np.sum(abs_error <= 5.0)\n",
    "\n",
    "        # Calculate percentages\n",
    "        pct_1_percent = (count_1_percent / total_count) * 100\n",
    "        pct_5_percent = (count_5_percent / total_count) * 100\n",
    "\n",
    "        results.append({\n",
    "            'Nodes': nodes,\n",
    "            'Total': total_count,\n",
    "            'Count_1pct': count_1_percent,\n",
    "            'Count_5pct': count_5_percent,\n",
    "            'Percentage_1pct': pct_1_percent,\n",
    "            'Percentage_5pct': pct_5_percent\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Create the plot\n",
    "fig = go.Figure()\n",
    "\n",
    "dataset = \"entire\"\n",
    "for model_type in results_all_table:\n",
    "\n",
    "    list_of_runs = results_all_table[model_type].keys()\n",
    "    if not list_of_runs:\n",
    "        continue\n",
    "\n",
    "    all_df = pd.DataFrame()\n",
    "    for run_id in list_of_runs:\n",
    "        if dataset not in results_all_table[model_type][run_id]:\n",
    "            continue\n",
    "\n",
    "        df = results_all_table[model_type][run_id][dataset]\n",
    "        all_df = pd.concat([all_df, df])\n",
    "\n",
    "    all_df = all_df.groupby(\"Graph\", as_index=False).agg({\"Error %\": \"mean\", **{col: 'first' for col in df.columns if col not in [\"Error %\"]}})\n",
    "\n",
    "    # Calculate the percentages\n",
    "    error_percentages = calculate_error_percentages(all_df)\n",
    "\n",
    "    # Add line for <= 1% error\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=error_percentages['Nodes'],\n",
    "        y=error_percentages['Percentage_1pct'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model_type} | ≤ 1%',\n",
    "        line=dict(color=colors[model_type], dash='solid'),\n",
    "        hovertemplate=(\n",
    "            'Nodes: %{x}<br>' +\n",
    "            'Percentage ≤ 1%: %{y:.2f}%<br>' +\n",
    "            'Count: %{customdata[0]}/%{customdata[1]}<br>' +\n",
    "            '<extra></extra>'\n",
    "        ),\n",
    "        customdata=np.column_stack([error_percentages['Count_1pct'], error_percentages['Total']])\n",
    "    ))\n",
    "\n",
    "    # Add line for <= 5% error\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=error_percentages['Nodes'],\n",
    "        y=error_percentages['Percentage_5pct'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model_type} | ≤ 5%',\n",
    "        legend=\"legend2\",\n",
    "        line=dict(color=colors[model_type], dash='dot'),\n",
    "        hovertemplate=(\n",
    "            'Nodes: %{x}<br>' +\n",
    "            'Percentage ≤ 5%: %{y:.2f}%<br>' +\n",
    "            'Count: %{customdata[0]}/%{customdata[1]}<br>' +\n",
    "            '<extra></extra>'\n",
    "        ),\n",
    "        customdata=np.column_stack([error_percentages['Count_5pct'], error_percentages['Total']])\n",
    "    ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Number of Nodes',\n",
    "        yaxis_title='% of graphs within error',\n",
    "        hovermode='x unified',\n",
    "        showlegend=True,\n",
    "        font=dict(size=12),\n",
    "        yaxis=dict(range=[0, 100])  # Set y-axis from 0 to 100%\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=5, r=5, t=5, b=65),\n",
    "    legend=dict(\n",
    "        title=dict(text=\"Model | Error threshold\", side=\"left\"),\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.14,\n",
    "        xanchor=\"right\",\n",
    "        x=1,\n",
    "        ),\n",
    "    legend2=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1,\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        tickmode=\"linear\",\n",
    "        tick0=0,\n",
    "        dtick=5,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        tickmode=\"linear\",\n",
    "        tick0=0,\n",
    "        dtick=10,\n",
    "    )\n",
    ")\n",
    "fig.show()\n",
    "image_path = pathlib.Path().cwd().parent / \"results\" / \"error_plots\"\n",
    "image_path.mkdir(parents=True, exist_ok=True)\n",
    "image_path /= f\"accuracy_per_size{save_ext}.pdf\"\n",
    "fig.write_image(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4502d621",
   "metadata": {},
   "source": [
    "# Plot errors per graph size\n",
    "For each recorded dataset (test or entire) and model type (full, dist. 32, dist. 64), we take the results from all seeds and average the error on each graph over those 20 seeds. We then plot the distribution of errors as if it were a single experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_fiedler_approx.gnn_utils.utils import create_combined_histogram\n",
    "\n",
    "for dataset in datasets_for_evaluation:\n",
    "    for model_type in results_all_table:\n",
    "\n",
    "        list_of_runs = results_all_table[model_type].keys()\n",
    "        if not list_of_runs:\n",
    "            continue\n",
    "\n",
    "        all_df = pd.DataFrame()\n",
    "        for run_id in list_of_runs:\n",
    "            if dataset not in results_all_table[model_type][run_id]:\n",
    "                continue\n",
    "\n",
    "            df = results_all_table[model_type][run_id][dataset]\n",
    "            all_df = pd.concat([all_df, df])\n",
    "\n",
    "        all_df = all_df.groupby(\"Graph\", as_index=False).agg({\"Error %\": \"mean\", **{col: 'first' for col in df.columns if col not in [\"Error %\"]}})\n",
    "\n",
    "        fig = create_combined_histogram(all_df, \"Nodes\", \"Error %\", title=f\"{model_type} - {dataset}\", xlabel=\"Number of nodes\", option=\"ci\")\n",
    "        fig.show()\n",
    "\n",
    "        if model_type == \"Full\" and dataset == \"test\":\n",
    "            fig.update_layout(\n",
    "                font=dict(size=20),\n",
    "                margin=dict(l=5, r=5, t=5, b=65),\n",
    "                legend=dict(\n",
    "                        # title=dict(text=\"Model - Dataset\", side=\"top\"),\n",
    "                        orientation=\"h\",\n",
    "                        yanchor=\"bottom\",\n",
    "                        y=1.02,\n",
    "                        xanchor=\"right\",\n",
    "                        x=1,\n",
    "                    ),\n",
    "            )\n",
    "            fig.write_image(pathlib.Path().cwd().parent / \"results\" / \"error_plots\" / f\"histogram_{model_type}_{dataset}{save_ext}.pdf\")\n",
    "\n",
    "            fig.update_layout(\n",
    "                yaxis2=dict(\n",
    "                    range=[-5, 5],\n",
    "                    tickvals=list(range(-5, 6, 1)),\n",
    "                    )\n",
    "            )\n",
    "            fig.write_image(pathlib.Path().cwd().parent / \"results\" / \"error_plots\" / f\"histogram_{model_type}_{dataset}_zoomed{save_ext}.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e0d5e",
   "metadata": {},
   "source": [
    "# Create the latex table with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f3d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Data Extraction and Processing ---\n",
    "dataset_keys = datasets_for_evaluation\n",
    "# Process in reverse to have 1% error threshold at the top of the table\n",
    "good_within_keys = [\"99\", \"95\", \"90\", \"80\"]\n",
    "\n",
    "processed_results = {}\n",
    "\n",
    "for model_type in sweep_ids.keys():\n",
    "    processed_results[model_type] = {}\n",
    "    for dataset in dataset_keys:\n",
    "        processed_results[model_type][dataset] = {}\n",
    "        for key in good_within_keys:\n",
    "            processed_results[model_type][dataset][key] = []\n",
    "            # Check if the model_type and its runs exist in the results\n",
    "            if model_type in results_all_scores and results_all_scores[model_type]:\n",
    "                for run_results in results_all_scores[model_type].values():\n",
    "                    # Check if the dataset and key exist for the run\n",
    "                    if dataset in run_results and key in run_results[dataset]:\n",
    "                        processed_results[model_type][dataset][key].append(run_results[dataset][key])\n",
    "\n",
    "# --- Table Generation ---\n",
    "\n",
    "def create_latex_table(df, caption, label):\n",
    "    \"\"\"Generates LaTeX code for a given DataFrame.\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    # Escape the '%' sign in the index for LaTeX rendering\n",
    "    df_copy.index = df_copy.index.str.replace('%', r'\\%', regex=False)\n",
    "\n",
    "    with pd.option_context('display.max_colwidth', None):\n",
    "        latex_str = df_copy.to_latex(\n",
    "            multicolumn_format='c',\n",
    "            multirow=True,\n",
    "            escape=False,\n",
    "            caption=caption,\n",
    "            label=label,\n",
    "            column_format='l' + 'c' * (len(df.columns))\n",
    "        )\n",
    "    latex_str = latex_str.replace(\"\\\\begin{table}\", \"\\\\begin{table}[htbp]\")\n",
    "    return latex_str\n",
    "\n",
    "def format_df_for_latex(df_numeric):\n",
    "    \"\"\"Converts a numeric DataFrame with mean/std columns to a LaTeX-formatted DataFrame.\"\"\"\n",
    "    # Create the column structure for the new LaTeX DataFrame\n",
    "    new_cols = df_numeric.columns.droplevel(2).unique()\n",
    "    df_latex = pd.DataFrame(index=df_numeric.index, columns=new_cols)\n",
    "\n",
    "    for model, dataset in new_cols:\n",
    "        # Ensure the mean/std columns exist before trying to access them\n",
    "        if (model, dataset, 'mean') in df_numeric.columns and (model, dataset, 'std') in df_numeric.columns:\n",
    "            mean_col = df_numeric[(model, dataset, 'mean')]\n",
    "            std_col = df_numeric[(model, dataset, 'std')]\n",
    "            # Format into \"mean ± std\" string, handling potential missing values\n",
    "            df_latex[(model, dataset)] = [\n",
    "                f\"${m:.2f} \\\\pm {s:.2f}$\" if pd.notna(m) and pd.notna(s) else \"N/A\"\n",
    "                for m, s in zip(mean_col, std_col)\n",
    "            ]\n",
    "    return df_latex\n",
    "\n",
    "# --- Create Numerical Tables ---\n",
    "\n",
    "# Table 1: Full Model (Numerical)\n",
    "data_full_num = []\n",
    "for key in good_within_keys:\n",
    "    row = [f\"{100 - int(key)}%\"]\n",
    "    for dataset in dataset_keys:\n",
    "        values = processed_results.get(\"Full\", {}).get(dataset, {}).get(key, [])\n",
    "        row.extend([np.mean(values), np.std(values)] if values else [np.nan, np.nan])\n",
    "    data_full_num.append(row)\n",
    "\n",
    "cols_full = pd.MultiIndex.from_product([['Full'], dataset_keys, ['mean', 'std']], names=['Model', 'Dataset', 'Stat'])\n",
    "df_full_num = pd.DataFrame(data_full_num, columns=['Error threshold'] + list(range(len(cols_full)))).set_index('Error threshold')\n",
    "df_full_num.columns = cols_full\n",
    "\n",
    "# Table 2: Dist. 32 & Dist. 64 Models (Numerical)\n",
    "data_dist_num = []\n",
    "for key in good_within_keys:\n",
    "    row = [f\"{100 - int(key)}%\"]\n",
    "    for model_type in [\"Dist. 32\", \"Dist. 64\"]:\n",
    "        for dataset in dataset_keys:\n",
    "            values = processed_results.get(model_type, {}).get(dataset, {}).get(key, [])\n",
    "            row.extend([np.mean(values), np.std(values)] if values else [np.nan, np.nan])\n",
    "    data_dist_num.append(row)\n",
    "\n",
    "cols_dist = pd.MultiIndex.from_product([[\"Dist. 32\", \"Dist. 64\"], dataset_keys, ['mean', 'std']], names=['Model', 'Dataset', 'Stat'])\n",
    "df_dist_num = pd.DataFrame(data_dist_num, columns=['Error threshold'] + list(range(len(cols_dist)))).set_index('Error threshold')\n",
    "df_dist_num.columns = cols_dist\n",
    "\n",
    "# Table 3: Combined Table (Numerical)\n",
    "df_combined_num = pd.concat([df_full_num, df_dist_num], axis=1)\n",
    "df_combined_num = df_combined_num.sort_index(axis=1, level=[0, 1])\n",
    "\n",
    "\n",
    "# --- Display Numerical Tables ---\n",
    "display(Markdown(\"### Full Model (Numerical)\"))\n",
    "display(df_full_num.style.format(precision=2, na_rep=\"N/A\"))\n",
    "display(Markdown(\"### Distributed Models (Numerical)\"))\n",
    "display(df_dist_num.style.format(precision=2, na_rep=\"N/A\"))\n",
    "display(Markdown(\"### Combined Models (Numerical)\"))\n",
    "display(df_combined_num.style.format(precision=2, na_rep=\"N/A\"))\n",
    "\n",
    "\n",
    "# --- Generate and Save LaTeX ---\n",
    "df_full_latex = format_df_for_latex(df_full_num)\n",
    "df_dist_latex = format_df_for_latex(df_dist_num)\n",
    "df_combined_latex = format_df_for_latex(df_combined_num)\n",
    "\n",
    "latex_full = create_latex_table(df_full_latex, \"Performance of the Full model.\", \"tab:full_model\")\n",
    "latex_dist = create_latex_table(df_dist_latex, \"Performance of the Distributed models.\", \"tab:dist_models\")\n",
    "latex_combined = create_latex_table(df_combined_latex, \"Combined performance of all models.\", \"tab:combined_models\")\n",
    "\n",
    "output_path = pathlib.Path().cwd().parent / \"results\" / \"latex_tables\"\n",
    "output_path.mkdir(exist_ok=True)\n",
    "with open(output_path / \"full.tex\", \"w\") as f:\n",
    "    f.write(latex_full)\n",
    "\n",
    "with open(output_path / \"dist.tex\", \"w\") as f:\n",
    "    f.write(latex_dist)\n",
    "\n",
    "with open(output_path / \"combined.tex\", \"w\") as f:\n",
    "    f.write(latex_combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
